#!/usr/bin/env python

#
# This commands template needs customization wherever you see CHANGES
# REQUIRED. Where you see CHANGES RECOMMENDED, check that section to
# make sure it works for your pipeline.
#

from __future__ import print_function

import contextlib
import gzip
import functools
import itertools
import os
import re
import sys


REFERENCE_ROOT = os.getenv('MKA_REFERENCE_ROOT', '/nfs/turbo/parkerlab1/lab/data/reference')

prefix_reference_root = functools.partial(os.path.join, REFERENCE_ROOT)

BWA_REFERENCES = {
    'dm3': prefix_reference_root('fly/dm3/index/bwa/0.7.12/dm3'),
    'dm6': prefix_reference_root('fly/dm6/index/bwa/0.7.12/dm6'),
    'hg19': prefix_reference_root('human/hg19/index/bwa/0.7.12/hg19'),
    'mm9': prefix_reference_root('mouse/mm9/index/bwa/0.7.12/mm9'),
    'mm10': prefix_reference_root('mouse/mm10/index/bwa/0.7.12/mm10'),
    'rn5': prefix_reference_root('rat/rn5/index/bwa/0.7.12/rn5'),
}

AUTOSOMAL_REFERENCES = {
    'dm3': ['chr2L', 'chr2LHet', 'chr2R', 'chr2RHet', 'chr3L', 'chr3LHet', 'chr3R', 'chr3RHet', 'chr4'],
    'dm6': ['chr2L', 'chr2R', 'chr3L', 'chr3R', 'chr4'],
    'hg19': ['chr{}'.format(i) for i in range(1, 23)],
    'mm9': ['chr{}'.format(i) for i in range(1, 20)],
    'mm10': ['chr{}'.format(i) for i in range(1, 20)],
    'rn5': ['chr{}'.format(i) for i in range(1, 21)],
}

EXCLUDED_REGIONS = {
    'dm3': map(prefix_reference_root, ['fly/dm3/annot/dm3-blacklist.bed.gz']),
    'hg19': map(prefix_reference_root, ['human/hg19/annot/wgEncodeDukeMapabilityRegionsExcludable.bed.gz', '/lab/human/hg19/annot/wgEncodeDacMapabilityConsensusExcludable.bed.gz']),
    'mm9': map(prefix_reference_root, ['mouse/mm9/annot/mm9-blacklist.bed.gz']),
    'mm10': map(prefix_reference_root, ['mouse/mm10/annot/mm10.blacklist.bed.gz'])
}

FASTQ_RE = re.compile('\.f(ast)?q(\.gz)?$')

MACS2_GENOME_SIZES = {
    'dm3': 'dm',
    'hg19': 'hs',
    'mm9': 'mm',
    'mm10': 'mm',
    'rn5': 'mm'
}

ORGANISMS = {
    'dm3': 'fly',
    'dm6': 'fly',
    'hg19': 'human',
    'mm9': 'mouse',
    'mm10': 'mouse',
    'rn5': 'rat'
}

ANALYSIS_NAME = "{{ANALYSIS_NAME}}"
DESCRIPTION = """{{DESCRIPTION}}"""
CONTROL_PATH = "{{CONTROL_PATH}}"
ANALYSIS_PATH = "{{ANALYSIS_PATH}}"
DATA_PATH = os.path.join(ANALYSIS_PATH, 'data')
WORK_PATH = os.path.join(ANALYSIS_PATH, 'work')
PIPELINE = os.path.join(ANALYSIS_PATH, 'pipeline')

ATAQV_DIR = os.path.join(WORK_PATH, 'ataqv')
BWA_DIR = os.path.join(WORK_PATH, 'bwa')
FASTQC_DIR = os.path.join(WORK_PATH, 'fastqc')
MACS2_DIR = os.path.join(WORK_PATH, 'macs2')
MD_DIR = os.path.join(WORK_PATH, 'mark_duplicates')
PRUNE_DIR = os.path.join(WORK_PATH, 'prune')
TRIM_ADAPTER_DIR = os.path.join(WORK_PATH, 'trim_adapters')

# By default, we use ionice and limit the number of particularly
# I/O-intensive jobs that run at once, to keep the machine
# responsive. If you're running on dedicated cluster nodes, you
# probably want to set this to 0.
LIMIT_IO = 8

#
# Library dictionary
#

LIBRARIES = {{LIBRARIES}}

SAMPLES = {}
for library in LIBRARIES.values():
    SAMPLES.setdefault(library['sample'], []).append(library)


def maybe_gzip(filename, ioniced=False):
    """Compress a file with gzip."""
    template_data = {
        'f': filename,
        'ionice': ioniced and 'ionice -c 3 ' or ''
    }

    command_template = """if [ -r "{f}" ]; then {ionice}gzip -f "{f}"; elif [ -r "{f}".gz ]; then echo '"{f}" already gzipped.'; fi"""

    printp(command_template.format(**template_data))


def mkdir(dir, mode=0o0750):
    """Construct a directory hierarchy using the given permissions."""
    if not os.path.exists(dir):
        os.makedirs(dir, mode)


def open_maybe_gzipped(filename):
    """
    Open a possibly gzipped file.

    Parameters
    ----------
    filename: str
        The name of the file to open.

    Returns
    -------
    file
        An open file object.
    """
    with open(filename, 'rb') as test_read:
        byte1, byte2 = test_read.read(1), test_read.read(1)
        if byte1 and ord(byte1) == 0x1f and byte2 and ord(byte2) == 0x8b:
            f = gzip.open(filename, mode='rt')
        else:
            f = open(filename, 'rt')
    return f


LEADING_WHITESPACE_RE = re.compile(r'(\s+)*(.*)')
def print_to_pipeline(pipeline_file, text=None, timed=False, ioniced=False):
    """The primary function of all this: writing to a drmr script."""
    if text:
        m = LEADING_WHITESPACE_RE.match(text)
        if m.group(1):
            pipeline_file.write(m.group(1))
        if timed:
            pipeline_file.write('/usr/bin/time -v ')
        if ioniced:
            pipeline_file.write('ionice -c3 ')
        pipeline_file.write(m.group(2))
        pipeline_file.write('\n')


@contextlib.contextmanager
def working_directory(path):
    """Changes to the given directory, returning to the original working directory when the context block is exited."""
    original_directory = os.getcwd()
    try:
        os.chdir(path)
        yield
    finally:
        os.chdir(original_directory)


def symlink(source_path, dest_path, abs=False):
    """Create a symbolic link from the source_path to the dest_path, which can be a directory."""

    workdir = os.path.isdir(dest_path) and dest_path or os.path.dirname(dest_path)

    with working_directory(workdir):
        src = abs and os.path.abspath(source_path) or os.path.relpath(source_path, dest_path)
        dest = dest_path
        dest_base = os.path.basename(dest)
        if os.path.isdir(dest_path):
            dest = os.path.join(dest_path, os.path.basename(src))
            if os.path.lexists(dest):
                os.unlink(dest)
            os.symlink(src, dest)
        else:
            mkdir(os.path.dirname(dest_path))
            if os.path.lexists(dest):
                os.unlink(dest)
            os.symlink(src, dest)
        return dest, dest_base


def rename_fastq(fastq, suffix=''):
    return FASTQ_RE.sub(fastq, suffix)


def iterate_library_source_files(library_name):
    """Generates a list of the library's original files."""
    library = LIBRARIES[library_name]
    for rg, files in library['readgroups'].items():
        for f in files:
            yield f


def iterate_all_source_files():
    return itertools.chain(*[iterate_library_source_files(library_name) for library_name in LIBRARIES.keys()])


def iterate_library_files(library_name):
    """Generates a list of the library's files in DATA_PATH."""
    library = LIBRARIES[library_name]
    for rg, files in library['readgroups'].items():
        for f in files:
            yield os.path.join(DATA_PATH, os.path.basename(f))


def iterate_all_files():
    return itertools.chain(*[iterate_library_files(library_name) for library_name in LIBRARIES.keys()])


def make_read_group_file(library_name, readgroup, suffix=''):
    return '{library_name}___{readgroup}{suffix}'.format(**locals())


def make_read_group_header(library, id):
    read_group_components = {
        'ID': '{}___{}'.format(library['library'], id),

        # library
        'LB': library['library'],

        # sample
        'SM': library['sample'],

        # sequencing center name
        'CN': library['sequencing_center'],

        # ISO8601 date(time) of sequencing
        'DT': library['sequencing_date'],

        # platform (Illumina, Solid, etc. -- see the spec for valid values
        'PL': library['sequencing_platform'],

        # free-form description
        'DS': library['description'],
    }

    header = """@RG\\t{}""".format('\\t'.join('{}:{}'.format(k, v) for k, v in read_group_components.items() if v))

    return header


def fastqc():
    """Run FastQC on all input libraries."""

    mkdir(FASTQC_DIR)

    printp("""\n#\n# run FastQC on initial data\n#""")
    printp("""\n# drmr:label fastqc""")
    printp("""\n# drmr:job time_limit=2h working_directory={}""".format(FASTQC_DIR))

    for index, f in enumerate(iterate_all_files(), 1):
        symlink(f, FASTQC_DIR)
        printp("""fastqc {}""".format(os.path.basename(f)), timed=True, ioniced=True)
        if LIMIT_IO and index % LIMIT_IO == 0:
            # limit the number of concurrent jobs to avoid thrashing the disk (set LIMIT_IO=False on clusters!)
            printp("""\n# drmr:wait""")


def trim_adapters():
    #
    # Trim adapter sequence from the FASTQ files
    #

    mkdir(TRIM_ADAPTER_DIR)

    printp("""\n#\n# trim adapter sequence from reads\n#""")
    printp("""\n# drmr:label trim-adapters""")
    printp("""\n# drmr:job time_limit=4h working_directory={}""".format(TRIM_ADAPTER_DIR))

    index = 0
    for name, library in LIBRARIES.items():
        for rg, files in library['readgroups'].items():
            for f in files:
                symlink(os.path.join(DATA_PATH, os.path.basename(f)), TRIM_ADAPTER_DIR)
            bases = [os.path.basename(f) for f in files]
            printp("""cta {} {}""".format(*bases), timed=True, ioniced=True)
            index += 1

            if LIMIT_IO and index % LIMIT_IO == 0:
                # limit the number of concurrent jobs to avoid thrashing the disk (set LIMIT_IO=False on clusters!)
                printp("""\n# drmr:wait""")


def bwa(threads=4, algorithm='MEM', time_limit='8h'):
    """
    Aligns reads to the reference genome with BWA.

    The BWA algorithm can be specified as 'MEM', 'backtrack', or
    'auto', to choose the algorithm based on the library read size.
    """

    mkdir(BWA_DIR)

    printp("""# drmr:label bwa\n""")
    printp("""# drmr:job nodes=1 processors={} working_directory={} time_limit={}""".format(threads, BWA_DIR, time_limit))

    backtrack_readgroups = []
    for library_name, library in LIBRARIES.items():
        reference_genome = library['reference_genome']
        bwa_reference = BWA_REFERENCES[reference_genome]

        for rg, files in library['readgroups'].items():
            fastq_files = [rename_fastq('.trimmed.fq.gz', os.path.basename(f)) for f in files]
            for f in fastq_files:
                symlink(os.path.join(TRIM_ADAPTER_DIR, f), BWA_DIR)

            if algorithm == 'auto':
                algorithm = 'MEM'
                try:
                    with open_maybe_gzipped(files[0]) as f:
                        next(f)
                        seq = next(f)
                        read_length = len(seq)
                        if read_length < 70:
                            algorithm = 'backtrack'
                    printp("""# Library {} read group {} read length: {}""".format(library_name, rg, read_length))
                except Exception as e:
                    printp("""# Could not determine read length for library {} read group {}: {}""".format(library_name, rg, e))

                printp("""# Using bwa's {} algorithm.""".format(algorithm))

            printp("""\n#\n# align reads in {library_name} read group {rg} to {reference_genome} with bwa {algorithm}\n#\n""".format(**locals()))

            if algorithm == 'MEM':
                bwa_input_files = ' '.join(fastq_files)
                read_group_header = make_read_group_header(library, rg)
                bam = make_read_group_file(library_name, rg, '.bam')
                printp("""bwa mem -M -R '{read_group_header}' -t {threads} {bwa_reference} {bwa_input_files} | samtools sort -m 1g -@ {threads} -O bam -T {library_name}___{rg}.sort -o {bam} -""".format(**locals()), timed=True, ioniced=True)

            elif algorithm == 'backtrack':
                backtrack_readgroups.append((library, rg, fastq_files))
                for input_file in fastq_files:
                    output_file = input_file.replace('.trimmed.fq.gz', '.sai')
                    printp("""bwa aln -t {threads} -f {output_file} {bwa_reference} {input_file}""".format(**locals()), timed=True, ioniced=True)

    printp("""\n# drmr:wait""")

    if backtrack_readgroups:
        printp("""\n#\n# Create the BAM files.\n#\n""")
        printp("""# drmr:job nodes=1 processors={} working_directory={} time_limit={}""".format(threads, BWA_DIR, time_limit))

    for library, rg, fastq_files in backtrack_readgroups:
        if len(fastq_files) == 1:
            bwa_command = 'samse'
        elif len(fastq_files) == 2:
            bwa_command = 'sampe'
        else:
            print('Too many FASTQ files for library {}'.format(library['library']), file=sys.stderr)
            sys.exit(1)

        fastq = ' '.join(fastq_files)
        sai = ' '.join([f.replace('.trimmed.fq.gz', '.sai') for f in fastq_files])
        read_group_header = make_read_group_header(library, rg)
        bam = make_read_group_file(library['library'], rg, '.bam')
        printp("""bwa {bwa_command} -r '{read_group_header}' {bwa_reference} {sai} {fastq} | samtools sort -m 1g -@ {threads} -O bam -T {library_name}___{rg}.sort -o {bam}""".format(**locals()), timed=True, ioniced=True)

        printp("""\n# drmr:wait""")

        printp("""\n#\n# Delete output files which are not needed after bwa has run\n#""")
        printp("""\n# drmr:label clean-after-bwa""")

        printp("""\n# drmr:job time_limit=00:15:00 working_directory={}""".format(BWA_DIR))
        printp("""rm -f {}/*.sai""".format(BWA_DIR))


def mark_duplicates():
    """
    Mark duplicates in each library BAM file.
    """

    mkdir(MD_DIR)

    printp("""# drmr:label mark-duplicates\n""")
    printp("""# drmr:job nodes=1 processors=2 memory=8g working_directory={} time_limit=8h""".format(MD_DIR))

    for library_name, library in LIBRARIES.items():
        for rg in library['readgroups'].keys():
            input_bam = make_read_group_file(library_name, rg, '.bam')
            output_bam = make_read_group_file(library_name, rg, '.md.bam')
            symlink(os.path.join(BWA_DIR, input_bam), MD_DIR)
            printp("""java -Xms6g -Xmx6g -XX:ParallelGCThreads=2 -jar $PICARD_HOME/picard.jar MarkDuplicates I={input_bam} O={output_bam} ASSUME_SORTED=true METRICS_FILE={library_name}___{rg}.markdup.metrics VALIDATION_STRINGENCY=LENIENT TMP_DIR=.""".format(**locals()), timed=True)

    printp("""\n# drmr:wait""")
    printp("""\n#\n# index the BAM files with marked duplicates, for pruning\n#\n""")
    printp("""\n# drmr:label index-md-bams\n""")

    for library_name, library in LIBRARIES.items():
        for rg in library['readgroups'].keys():
            bam = make_read_group_file(library_name, rg, '.md.bam')
            printp("""samtools index {bam}""".format(**locals()), timed=True)


def prune(mapq=30):
    """
    Prune the BAM files down to properly paired and uniquely mapped
    autosomal alignments with good quality, and remove all duplicates
    """

    mkdir(PRUNE_DIR)

    printp("""\n# drmr:label prune\n""")
    printp("""# drmr:job nodes=1 processors=1 memory=4g time_limit=4h working_directory={}""".format(PRUNE_DIR))

    for library_name, library in LIBRARIES.items():
        for rg in library['readgroups'].keys():
            symlink(os.path.join(MD_DIR, make_read_group_file(library_name, rg, '.md.bam')), PRUNE_DIR)
            symlink(os.path.join(MD_DIR, make_read_group_file(library_name, rg, '.md.bam.bai')), PRUNE_DIR)

    #
    # samtools filters:
    #   -f 3: keep properly paired and mapped reads
    #   -F 4: filter out unmapped reads
    #   -F 8: filter out unmapped mates
    #   -F 256: filter out secondary reads
    #   -F 1024: filter out duplicates marked by Picard above
    #   -F 2048: filter out supplementary reads
    #
    printp("""\n#\n# prune the BAM files with marked duplicates down to properly paired""")
    printp("""# and mapped primary autosomal alignments of good quality, for peak calling\n#\n""")

    PRUNE_TEMPLATE = """samtools view -b -h -f 3 -F 4 -F 8 -F 256 -F 1024 -F 2048 -q {mapq} {input_bam} {autosomes} > {output_bam}"""

    for library_name, library in LIBRARIES.items():
        autosomes = ' '.join(AUTOSOMAL_REFERENCES[library['reference_genome']])
        for rg in library['readgroups'].keys():
            input_bam = make_read_group_file(library_name, rg, '.md.bam')
            output_bam = make_read_group_file(library_name, rg, '.pruned.bam')
            printp(PRUNE_TEMPLATE.format(**locals()), timed=True)


def compress_macs2_output(prefix, ioniced=True):
    maybe_gzip("{}.broad_peaks.broadPeak".format(prefix), ioniced)
    maybe_gzip("{}.broad_peaks.gappedPeak".format(prefix), ioniced)
    maybe_gzip("{}.broad_control_lambda.bdg".format(prefix), ioniced)
    maybe_gzip("{}.broad_treat_pileup.bdg".format(prefix), ioniced)


def exclude_blacklisted_peaks(prefix, reference_genome):
    exclude_commands = ' | '.join(['intersectBed -a stdin -b {} -v'.format(erf) for erf in EXCLUDED_REGIONS[reference_genome]])
    printp("""zcat {prefix}.broad_peaks.broadPeak.gz | {exclude_commands} | gzip -c > {prefix}.broad_peaks.broadPeak.noblacklist.gz""".format(**locals()))


def macs2(shift=-100, extsize=200, exclude_blacklisted_regions=False, call_libraries=False, call_samples=False):
    """
    Call peaks with MACS2.
    """

    mkdir(MACS2_DIR)

    printp("""\n#\n# peak calling\n#""")

    printp("""\n#\n# call broad peaks (keeping dups in case of merged files)\n#\n""")
    printp("""\n# drmr:job nodes=1 processors=1 memory=8g working_directory={} time_limit=4h""".format(MACS2_DIR))

    MACS2_BROAD_PEAK_TEMPLATE = """{% raw %}macs2 callpeak -t {{input_bam}} -f BAM -n {{prefix}}.broad -g {{macs2_genome_size}} --nomodel --shift {shift} --extsize {extsize} -B --broad --keep-dup all{% endraw %}""".format(**locals())

    printp("""# drmr:label macs2-readgroups\n""")
    for library_name, library in sorted(LIBRARIES.items()):
        macs2_genome_size = MACS2_GENOME_SIZES[library['reference_genome']]
        for rg in library['readgroups'].keys():
            input_bam = make_read_group_file(library_name, rg, '.pruned.bam')
            prefix = make_read_group_file(library_name, rg)
            symlink(os.path.join(PRUNE_DIR, input_bam), MACS2_DIR)
            printp(MACS2_BROAD_PEAK_TEMPLATE.format(**locals()), timed=True, ioniced=True)

    # If asked to call peaks on libraries, make a pruned BAM for each
    # then run MACS2 on those.
    if call_libraries:
        printp("""# drmr:job nodes=1 processors=1 memory=4g time_limit=4h working_directory={}""".format(PRUNE_DIR))
        printp("""\n# drmr:label make-pruned-libraries\n""")
        for index, (library_name, library) in enumerate(sorted(LIBRARIES.items()), 1):
            if LIMIT_IO and index % LIMIT_IO == 0:
                # limit the number of concurrent jobs to avoid thrashing the disk (set LIMIT_IO=False on clusters!)
                printp("""\n# drmr:wait""")

            printp("""\nsamtools merge -f {}.pruned.bam {}""".format(library_name, ' '.join(make_read_group_file(library_name, rg, '.pruned.bam') for rg in sorted(library['readgroups'].keys()))), ioniced=True)

        printp("""\n# drmr:wait""")

        printp("""\n# drmr:job nodes=1 processors=1 memory=8g working_directory={} time_limit=4h""".format(MACS2_DIR))
        printp("""\n# drmr:label macs2-libraries\n""")
        for library_name, library in sorted(LIBRARIES.items()):
            input_bam = '{}.pruned.bam'.format(library_name)
            prefix = library_name
            symlink(os.path.join(PRUNE_DIR, input_bam), MACS2_DIR)
            printp(MACS2_BROAD_PEAK_TEMPLATE.format(**locals()), timed=True, ioniced=True)

    # If asked to call peaks on samples, make a pruned BAM for each,
    # and run MACS2 on those.
    if call_samples:
        printp("""# drmr:job nodes=1 processors=1 memory=4g time_limit=4h working_directory={}""".format(PRUNE_DIR))
        printp("""\n# drmr:label make-pruned-samples\n""")
        for index, (sample, libraries) in enumerate(sorted(SAMPLES.items()), 1):
            if LIMIT_IO and index % LIMIT_IO == 0:
                # limit the number of concurrent jobs to avoid thrashing the disk (set LIMIT_IO=False on clusters!)
                printp("""\n# drmr:wait""")

            pruned_readgroup_bams = []
            for library in sorted(libraries):
                pruned_readgroup_bams.extend(make_read_group_file(library['library'], rg, '.pruned.bam') for rg in sorted(library['readgroups'].keys()))

            printp("""\nsamtools merge -f {}.pruned.bam {}""".format(sample, ' '.join(pruned_readgroup_bams)), ioniced=True)

        printp("""\n# drmr:wait""")

        printp("""\n# drmr:job nodes=1 processors=1 memory=8g working_directory={} time_limit=4h""".format(MACS2_DIR))
        printp("""\n# drmr:label macs2-samples\n""")
        for sample in sorted(SAMPLES.keys()):
            input_bam = '{}.pruned.bam'.format(sample)
            prefix = sample
            symlink(os.path.join(PRUNE_DIR, input_bam), MACS2_DIR)
            printp(MACS2_BROAD_PEAK_TEMPLATE.format(**locals()), timed=True, ioniced=True)

    printp("""\n# drmr:wait""")

    printp("""\n#\n# Compress MACS2 BedGraph output\n#""")
    printp("""\n# drmr:label compress-macs2-output\n""")

    for sample, libraries in sorted(SAMPLES.items()):
        if call_samples:
            compress_macs2_output(sample, ioniced=True)
        for library in sorted(libraries):
            if call_libraries:
                compress_macs2_output(library['library'], ioniced=True)
            for rg in sorted(library['readgroups'].keys()):
                compress_macs2_output(make_read_group_file(library['library'], rg), ioniced=True)

    if exclude_blacklisted_regions:
        printp("""\n# drmr:wait""")
        printp("""\n# drmr:label exclude-blacklisted-peaks\n""")
        for sample, libraries in sorted(SAMPLES.items()):
            if call_samples:
                exclude_blacklisted_peaks(sample, libraries[0]['reference_genome'])

            for library in sorted(libraries):
                if call_libraries:
                    exclude_blacklisted_peaks(library['library'], library['reference_genome'])

                for rg in sorted(library['readgroups'].keys()):
                    prefix = make_read_group_file(library['library'], rg)
                    exclude_blacklisted_peaks(prefix, library['reference_genome'])


def ataqv(use_autosomal_reference_files=False):
    """
    Run ataqv for each library.
    """

    mkdir(ATAQV_DIR)

    printp("""\n#\n# run ataqv\n#""")
    printp("""\n# drmr:label ataqv""")

    ATAQV_TEMPLATE = """ataqv --peak-file {peaks} --metrics-file {metrics_file} --name 'Library {library_name}, read group {rg}' {ataqv_options} {organism} {alignments} > {output_file}\n"""

    printp("""\n# drmr:job nodes=1 memory=8g working_directory={} time_limit=4h""".format(ATAQV_DIR))

    for library_name, library in LIBRARIES.items():
        if use_autosomal_reference_files:
            autosomal_reference_filename = os.path.join(ATAQV_DIR, '{}.arf'.format(library_name))
            with open(autosomal_reference_filename, 'w') as arf:
                for ar in AUTOSOMAL_REFERENCES[library['reference_genome']]:
                    arf.write(ar)
                    arf.write('\n')

        for rg in library['readgroups'].keys():
            alignments = make_read_group_file(library_name, rg, '.md.bam')
            peaks = make_read_group_file(library_name, rg, '.broad_peaks.broadPeak.gz')
            symlink(os.path.join(MD_DIR, alignments), ATAQV_DIR)
            symlink(os.path.join(MACS2_DIR, peaks), ATAQV_DIR)

            ataqv_options = ["""{} '{}'""".format(k, v) for k, v in {
                '--description': DESCRIPTION,
                '--library-description': library.get('description'),
                '--url': library.get('url'),
            }.items() if v]

            if use_autosomal_reference_files:
                ataqv_options.append('--autosomal-reference-file {}'.format(autosomal_reference_filename))

            for excluded_region_file in EXCLUDED_REGIONS[library['reference_genome']]:
                ataqv_options.append('--excluded-region-file {}'.format(excluded_region_file))

            ataqv_options = ' '.join(ataqv_options)

            organism = ORGANISMS.get(library['reference_genome'])

            metrics_file = make_read_group_file(library_name, rg, '.ataqv.json.gz')
            output_file = make_read_group_file(library_name, rg, '.ataqv.out')
            printp(ATAQV_TEMPLATE.format(**locals()), timed=True, ioniced=True)


if __name__ == '__main__':
    mkdir(WORK_PATH)
    mkdir(DATA_PATH)

    for source_file in iterate_all_source_files():
        dest = os.path.join(DATA_PATH, os.path.basename(source_file))
        symlink(source_file, dest, abs=True)

    if os.path.exists(PIPELINE):
        os.unlink(PIPELINE)

    PIPELINE_FILE = open(PIPELINE, 'w')
    printp = functools.partial(print_to_pipeline, PIPELINE_FILE)

    printp("""#!/bin/bash""")
    printp("""# -*- mode: sh; coding: utf-8 -*-\n""")

    fastqc()

    trim_adapters()

    printp("""\n# drmr:wait""")

    bwa()

    printp("""\n# drmr:wait""")

    mark_duplicates()

    printp("""\n# drmr:wait""")

    prune()

    printp("""\n# drmr:wait""")

    macs2(exclude_blacklisted_regions=True)

    printp("""\n# drmr:wait""")

    ataqv()
