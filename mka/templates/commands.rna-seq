#!/usr/bin/env python

#
# This analysis was generated with this mka command:
#
# {{MKA_COMMAND_LINE}}
#
# run in this directory:
#
# {{MKA_CWD}}
#

from __future__ import print_function

import contextlib
import functools
import itertools
import os
import re


REFERENCE_ROOT = os.getenv('MKA_REFERENCE_ROOT', '/lab/data/reference')

prefix_reference_root = functools.partial(os.path.join, REFERENCE_ROOT)

STAR_REFERENCES = {
    'danforth': prefix_reference_root('mouse/danforth/index/STAR/current'),
    'hg19': prefix_reference_root('human/hg19/index/STAR/current'),
    'mm9': prefix_reference_root('mouse/mm9/index/STAR/current'),
    'mm9etn': prefix_reference_root('mouse/mm9etn/index/STAR/current/sjdbOverhang100'),
    'rn5': prefix_reference_root('rat/rn5/index/STAR/current'),
}

FASTQ_RE = re.compile('\.f(ast)?q(\.gz)?$')

ANALYSIS_NAME = "{{ANALYSIS_NAME}}"
DESCRIPTION = """{{DESCRIPTION}}"""
CONTROL_PATH = "{{CONTROL_PATH}}"
ANALYSIS_PATH = "{{ANALYSIS_PATH}}"
DATA_PATH = os.path.join(ANALYSIS_PATH, 'data')
WORK_PATH = os.path.join(ANALYSIS_PATH, 'work')
PIPELINE = os.path.join(ANALYSIS_PATH, 'pipeline')

FASTQC_DIR = os.path.join(WORK_PATH, 'fastqc')
STAR_DIR = os.path.join(WORK_PATH, 'star')
QORTS_DIR = os.path.join(WORK_PATH, 'qorts')
TRACK_DIR = os.path.join(WORK_PATH, 'tracks')
TRIM_ADAPTER_DIR = os.path.join(WORK_PATH, 'trim_adapters')

# By default, we use ionice and limit the number of particularly
# I/O-intensive jobs that run at once, to keep the machine
# responsive. If you're running on dedicated cluster nodes, you
# probably want to set this to False.
LIMIT_IO = 8


#
# Library dictionary
#

LIBRARIES = {{LIBRARIES}}

SAMPLES = {}
for library in LIBRARIES.values():
    SAMPLES.setdefault(library['sample'], []).append(library)


def maybe_gzip(filename, ioniced=False):
    """Compress a file with gzip."""
    template_data = {
        'f': filename,
        'ionice': ioniced and 'ionice -c2 -n7 ' or ''
    }

    command_template = """if [ -r "{f}" ]; then {ionice}gzip -f "{f}"; elif [ -r "{f}".gz ]; then echo '"{f}" already gzipped.'; fi"""

    printp(command_template.format(**template_data))


def mkdir(dir, mode=0o0750):
    """Construct a directory hierarchy using the given permissions."""
    if not os.path.exists(dir):
        os.makedirs(dir, mode)


def print_to_pipeline(pipeline_file, text=None, timed=False, ioniced=False):
    """The primary function of all this: writing to a drmr script."""
    if text:
        if timed:
            pipeline_file.write('/usr/bin/time -v ')
        if ioniced:
            pipeline_file.write('ionice -c2 -n7 ')
        pipeline_file.write(text)
        pipeline_file.write('\n')


@contextlib.contextmanager
def working_directory(path):
    """Changes to the given directory, returning to the original working directory when the context block is exited."""
    original_directory = os.getcwd()
    try:
        os.chdir(path)
        yield
    finally:
        os.chdir(original_directory)


def symlink(source_path, dest_path, absolute=False):
    """Create a symbolic link from the source_path to the dest_path, which can be a directory."""

    workdir = os.path.isdir(dest_path) and dest_path or os.path.dirname(dest_path)

    with working_directory(workdir):
        src = os.path.normpath(absolute and os.path.abspath(source_path) or os.path.relpath(source_path, dest_path))
        dest = dest_path
        dest_base = os.path.basename(dest)
        if os.path.isdir(dest_path):
            dest = os.path.join(dest_path, os.path.basename(src))
            if os.path.lexists(dest):
                os.unlink(dest)
            os.symlink(src, dest)
        else:
            mkdir(os.path.dirname(dest_path))
            if os.path.lexists(dest):
                os.unlink(dest)
            os.symlink(src, dest)
        return dest, dest_base


def rename_fastq(fastq, suffix=''):
    return FASTQ_RE.sub(fastq, suffix)


def iterate_library_source_files(library_name):
    """Generates a list of the library's original files."""
    library = LIBRARIES[library_name]
    for rg, files in sorted(library['readgroups'].items()):
        for f in files:
            yield f


def iterate_all_source_files():
    return itertools.chain(*[iterate_library_source_files(library_name) for library_name in sorted(LIBRARIES.keys())])


def iterate_library_files(library_name):
    """Generates a list of the library's files in DATA_PATH."""
    library = LIBRARIES[library_name]
    for rg, files in sorted(library['readgroups'].items()):
        for f in files:
            yield os.path.join(DATA_PATH, os.path.basename(f))


def iterate_all_files():
    return itertools.chain(*[iterate_library_files(library_name) for library_name in sorted(LIBRARIES.keys())])


def library_reference_genomes():
    return sorted(list(set(library['reference_genome'] for library_name, library in sorted(LIBRARIES.items()))))


def libraries_by_genome():
    libraries = {}
    for genome in library_reference_genomes():
        libraries[genome] = [library for library_name, library in sorted(LIBRARIES.items()) if library['reference_genome'] == genome]

    # return genome, libraries for each genom
    return sorted(libraries.items())


def make_read_group_path(library_name, readgroup, suffix=''):
    return '{library_name}___{readgroup}{suffix}'.format(**locals())


def make_read_group_header(library, id):
    return """ID:"{}" LB:"{}" SM:"{}" CN:"{}" DT:"{}" PL:"{}" DS:"{}" """.format(
        '{}___{}'.format(library['library'], id),
        library['library'],
        library['sample'],
        library['sequencing_center'],
        library['sequencing_date'],
        library['sequencing_platform'],
        library['description'].replace('\n', ' ')
    )


def fastqc():
    """Run FastQC on all input libraries."""

    mkdir(FASTQC_DIR)

    printp("""\n#\n# run FastQC on initial data\n#""")
    printp("""\n# drmr:label fastqc""")
    printp("""\n# drmr:job time_limit=2h working_directory={}""".format(FASTQC_DIR))

    for index, f in enumerate(iterate_all_files(), 1):
        symlink(f, FASTQC_DIR)
        printp("""fastqc {}""".format(os.path.basename(f)), timed=True, ioniced=True)
        if LIMIT_IO and index % LIMIT_IO == 0:
            # limit the number of concurrent jobs to avoid thrashing the disk
            printp("""\n# drmr:wait""")


def trim_adapters():
    #
    # Trim adapter sequence from the FASTQ files
    #

    mkdir(TRIM_ADAPTER_DIR)

    printp("""\n#\n# trim adapter sequence from reads\n#""")
    printp("""\n# drmr:label trim-adapters""")
    printp("""\n# drmr:job time_limit=4h working_directory={}""".format(TRIM_ADAPTER_DIR))

    index = 0
    for name, library in sorted(LIBRARIES.items()):
        for rg, files in sorted(library['readgroups'].items()):
            for f in sorted(files):
                symlink(os.path.join(DATA_PATH, os.path.basename(f)), TRIM_ADAPTER_DIR)
            bases = ' '.join(os.path.basename(f) for f in sorted(files))
            printp("""cta {}""".format(bases), timed=True, ioniced=True)
            index += 1

            if LIMIT_IO and index % LIMIT_IO == 0:
                # limit the number of concurrent jobs to avoid thrashing the disk (set LIMIT_IO=False on clusters!)
                printp("""\n# drmr:wait""")

    printp("""\n# drmr:wait""")


def star(mapping_template=None, threads=16, memory=32, max_star_instances=0):
    if mapping_template is None:
        mapping_template = """STAR --runThreadN {} --readFilesCommand gunzip -cf --genomeDir {} --outSAMtype BAM SortedByCoordinate --outSAMunmapped Within KeepPairs --outSAMattrRGline {} --readFilesIn {}"""

    printp('\n# drmr:label star')

    index = 0
    for reference_genome, libraries in libraries_by_genome():
        star_reference = STAR_REFERENCES[reference_genome]

        for library in libraries:
            for rg, files in sorted(library['readgroups'].items()):
                fastq_files = sorted([rename_fastq('.trimmed.fq.gz', os.path.basename(f)) for f in files])
                star_dir = os.path.join(STAR_DIR, make_read_group_path(library['library'], rg))
                mkdir(star_dir)

                for f in fastq_files:
                    symlink(os.path.join(TRIM_ADAPTER_DIR, f), star_dir)

                fastq_files = [os.path.basename(f) for f in fastq_files]

                printp('\n# drmr:job time_limit=4h processors={} memory={}g working_directory={}'.format(threads, memory, star_dir))
                printp(mapping_template.format(threads, star_reference, make_read_group_header(library, rg), ' '.join(fastq_files)), timed=True, ioniced=True)

                index += 1
                if max_star_instances and index % max_star_instances == 0:
                    # limit the number of concurrent jobs to avoid thrashing the disk
                    printp("""\n# drmr:wait""")

    printp("""\n# drmr:wait""")


def merge_star_libraries():
    printp("""\n# drmr:label merge-star-libraries""")
    printp("""# drmr:job nodes=1 processors=1 memory=4g time_limit=4h working_directory={}""".format(STAR_DIR))

    libraries_dir = os.path.join(STAR_DIR, 'libraries')
    mkdir(libraries_dir)
    for index, (library_name, library) in enumerate(sorted(LIBRARIES.items()), 1):
        if LIMIT_IO and index % LIMIT_IO == 0:
            # limit the number of concurrent jobs to avoid thrashing the disk (set LIMIT_IO=False on clusters!)
            printp("""\n# drmr:wait""")

        readgroup_bams = [os.path.join(STAR_DIR, make_read_group_path(library['library'], rg), 'Aligned.sortedByCoord.out.bam') for rg in sorted(library['readgroups'].keys())]

        printp("""samtools merge -f {}/{}.bam {}""".format(libraries_dir, library_name, ' '.join(readgroup_bams)), ioniced=True)


def merge_star_samples():
    printp("""\n# drmr:label merge-star-samples""")
    printp("""# drmr:job nodes=1 processors=1 memory=4g time_limit=4h working_directory={}""".format(STAR_DIR))

    samples_dir = os.path.join(STAR_DIR, 'samples')
    mkdir(samples_dir)
    for index, (sample, libraries) in enumerate(sorted(SAMPLES.items()), 1):
        if LIMIT_IO and index % LIMIT_IO == 0:
            # limit the number of concurrent jobs to avoid thrashing the disk (set LIMIT_IO=False on clusters!)
            printp("""\n# drmr:wait""")

        sample_bams = []
        for library in libraries:
            readgroup_bams = [os.path.join(STAR_DIR, make_read_group_path(library['library'], rg), 'Aligned.sortedByCoord.out.bam') for rg in sorted(library['readgroups'].keys())]
            sample_bams.extend(readgroup_bams)

        printp("""samtools merge -f {}/{}.bam {}""".format(samples_dir, sample, ' '.join(sample_bams)), ioniced=True)


def qorts(command_template=None, threads=4, memory=12):
    if command_template is None:
        qorts_command_template = """java -Xmx8g -jar $QORTS_JAR QC {} --generatePlots --title {} --chromSizes {} {} {} {}"""

    printp('\n# drmr:label qorts')

    # run QoRTs on each library's BAM
    for library_name, library in sorted(LIBRARIES.items()):
        star_reference = STAR_REFERENCES[library['reference_genome']]
        chrom_sizes = os.path.join(star_reference, 'chrNameLength.txt')
        gtf = os.path.join(star_reference, 'annotation.gtf')

        strand_options = ''
        library_strand = library['analysis_specific_options'].get('strand')
        if library_strand == 'fr-firststrand':
            strand_options = '--stranded'
        elif library_strand == 'fr-secondstrand':
            strand_options = '--stranded --stranded_fr_secondstrand'

        for rg, files in sorted(library['readgroups'].items()):
            read_group_dir = make_read_group_path(library_name, rg)
            star_dir = os.path.join(STAR_DIR, read_group_dir)
            bam = os.path.join(star_dir, 'Aligned.sortedByCoord.out.bam')

            qorts_dir = os.path.join(QORTS_DIR, read_group_dir)
            mkdir(qorts_dir)

            printp('\n# drmr:job time_limit=2h processors={} memory={}g working_directory={}'.format(threads, memory, qorts_dir))
            printp(qorts_command_template.format(strand_options, library_name, chrom_sizes, bam, gtf, qorts_dir), timed=True, ioniced=True)

    printp("""\n# drmr:wait""")

    printp('\n# drmr:label qorts-multi')

    # plot all the things
    decoder_filename = os.path.join(QORTS_DIR, 'decoder.txt')
    with open(decoder_filename, 'w') as decoder:
        decoder.write('sample.ID\n')
        for library_name, library in LIBRARIES.items():
            for rg in sorted(library['readgroups'].keys()):
                read_group_dir = make_read_group_path(library_name, rg)
                decoder.write('{}\n'.format(read_group_dir))

    mkdir(os.path.join(QORTS_DIR, ANALYSIS_NAME))

    printp('\n# drmr:job time_limit=24h processors={} memory={}g working_directory={}'.format(threads, memory, QORTS_DIR))
    printp("""qortsGenMultiQC.R {}/ {} {}/""".format(QORTS_DIR, decoder_filename, ANALYSIS_NAME))


def qorts_wig2bigwig(command_template=None, memory=1):
    if command_template is None:
        command_template = """ionice -c3 wigToBigWig {wig} {chrom_sizes} {bigwig}"""

    wigs = ['QC.wiggle.fwd.wig.gz', 'QC.wiggle.rev.wig.gz']
    bigwigs = ['QC.wiggle.fwd.bw', 'QC.wiggle.rev.bw']

    printp('\n# drmr:label qorts-wig2bigwig')

    for library_name, library in sorted(LIBRARIES.items()):
        star_reference = STAR_REFERENCES[library['reference_genome']]
        chrom_sizes = os.path.join(star_reference, 'chrNameLength.txt')
        for rg in sorted(library['readgroups'].keys()):
            read_group_dir = make_read_group_path(library_name, rg)
            star_dir = os.path.join(STAR_DIR, read_group_dir)
            bam = os.path.join(star_dir, 'Aligned.sortedByCoord.out.bam')

            qorts_dir = os.path.join(QORTS_DIR, read_group_dir)

            printp('\n# drmr:job time_limit=1h processors=1 memory={}g working_directory={}'.format(memory, qorts_dir))
            for wig, bigwig in zip(wigs, bigwigs):
                printp(command_template.format(**locals()))


def make_r1ss_tracks():
    """Run merge_star_samples before this."""

    printp('\n#\n# Make read1 start site tracks\n#')
    printp('\n# drmr:label r1ss')

    mkdir(TRACK_DIR)
    samples_dir = os.path.join(STAR_DIR, 'samples')

    #
    # Index the sample BAMs
    #
    printp('\n# drmr:label index-sample-bams')
    printp('\n# drmr:job time_limit=1h processors=1 memory=8g working_directory={}'.format(samples_dir))
    for index, sample in enumerate(sorted(SAMPLES.keys()), 1):
        if LIMIT_IO and index % LIMIT_IO == 0:
            printp("""\n# drmr:wait""")

        sample_bam = '{}.bam'.format(sample)
        printp("samtools index " + os.path.basename(sample_bam))

    printp('\n# drmr:wait')

    strand_filters = {
        'fwd': '-f 99 -F 16',  # -f 99: read paired, read mapped in proper pair, mate reverse strand, first in pair; -F 16 not read reverse strand
        'rev': '-f 83 -F 32'   # -f 83: read paired, read mapped in proper pair, read reverse strand, first in pair; -F 32 not mate reverse strand
    }

    filter_template = """(export CHROMOSOMES=$(samtools view -H {input_bam} | grep '^@SQ' | cut -f 2 | grep -v -e s_ -e chrM -e chrY -e 'VN:' | sed 's/SN://' | xargs echo); samtools view -b -h {filters} -f 3 -F 4 -F 8 -q 255 {input_bam} $CHROMOSOMES > {output_bam})"""

    #
    # Prune the sample BAMs to quality 255 properly paired and mapped autosomal reads
    #
    printp('\n# drmr:label prune-sample-bams')
    printp('\n# drmr:job time_limit=1h processors=1 memory=8g working_directory={}'.format(TRACK_DIR))
    for index, sample in enumerate(sorted(SAMPLES.keys()), 1):
        if LIMIT_IO and index % LIMIT_IO == 0:
            printp("""\n# drmr:wait""")

        input_bam = '{}.bam'.format(sample)
        sample_bam = os.path.join(samples_dir, input_bam)
        sample_bam_index = sample_bam + '.bai'
        symlink(sample_bam, TRACK_DIR)
        symlink(sample_bam_index, TRACK_DIR)
        for strand in ['fwd', 'rev']:
            filters = strand_filters[strand]
            output_bam = '{sample}.read1.{strand}.pruned.bam'.format(**locals())
            printp(filter_template.format(**locals()))

    printp('\n# drmr:wait')

    #
    # Index the pruned BAMs
    #
    printp('\n# drmr:label index-pruned')
    for index, sample in enumerate(sorted(SAMPLES.keys()), 1):
        if LIMIT_IO and index % LIMIT_IO == 0:
            printp("""\n# drmr:wait""")

        for strand in ['fwd', 'rev']:
            bam = '{sample}.read1.{strand}.pruned.bam'.format(**locals())
            printp("samtools index " + bam)

    printp('\n# drmr:wait')

    printp('\n# drmr:label stats-pruned')
    for index, sample in enumerate(sorted(SAMPLES.keys()), 1):
        if LIMIT_IO and index % LIMIT_IO == 0:
            printp("""\n# drmr:wait""")

        for strand in ['fwd', 'rev']:
            input_bam = '{sample}.read1.{strand}.pruned.bam'.format(**locals())
            printp('samtools flagstat {input_bam} > {input_bam}.flagstat'.format(**locals()))

    printp('\n# drmr:label clip')
    printp('\n# drmr:job time_limit=1h processors=1 memory=4g working_directory={}'.format(TRACK_DIR))
    bamtobed_template = """bam clipOverlap --in {input_bam} --out -.bam | bamToBed -i /dev/stdin | sort -k 1,1 -k2,2n | {bed_start_site_conversion} > {output_bed} """

    start_site_converters = {
        'fwd': """awk 'BEGIN {{FS="\\t"; OFS="\\t";}} {{print $1, $2, ($2 + 1), $4, $5, $6;}}'""",
        'rev': """awk 'BEGIN {{FS="\\t"; OFS="\\t";}} {{print $1, ($3 - 1), $3, $4, $5, $6;}}'"""
    }

    #
    # convert the pruned BAMs to BEDs containing just read1 start sites
    #
    for index, sample in enumerate(sorted(SAMPLES.keys()), 1):
        if LIMIT_IO and index % LIMIT_IO == 0:
            printp("""\n# drmr:wait""")

        for strand in ['fwd', 'rev']:
            bed_start_site_conversion = start_site_converters[strand]
            input_bam = '{sample}.read1.{strand}.pruned.bam'.format(**locals())
            output_bed = '{sample}.read1.{strand}.pruned.bed'.format(**locals())
            template = bamtobed_template.format(**locals())
            printp(template.format(**locals()))

    printp('\n# drmr:wait')

    printp('\n#\n# Make the read1 start site tracks\n#')
    printp('\n# drmr:label r1ss-track')

    bedgraph_template = """export MAPPED_READ_COUNT=$(wc -l {input_bed} | awk '{{print $1}}'); export SCALE_FACTOR=$(perl -E "printf \"%f\\n\", ${{MAPPED_READ_COUNT}} / 1000000.0;");  {input_bed} | bedtools genomecov -scale ${{SCALE_FACTOR}} -bga -i {input_bed} -g {chrom_sizes} | LC_COLLATE=C sort -k1,1 -k2,2n > {output_bg}"""

    for index, (sample, libraries) in enumerate(sorted(SAMPLES.items()), 1):
        if LIMIT_IO and index % LIMIT_IO == 0:
            printp("""\n# drmr:wait""")

        star_reference = STAR_REFERENCES[libraries[0]['reference_genome']]
        chrom_sizes = os.path.join(star_reference, 'chrNameLength.txt')

        for strand in ['fwd', 'rev']:
            input_bed = '{sample}.read1.{strand}.pruned.bed'.format(**locals())
            output_bg = '{sample}.read1.{strand}.pruned.coverage.bg'.format(**locals())
            printp(bedgraph_template.format(**locals()))

    printp('\n# drmr:wait')

    negate_template = """awk 'BEGIN {{FS="\\t"; OFS="\\t";}} {{print $1, $2, $3, -1 * $4;}}' {input_bg} > {input_bg}.negated && mv {input_bg}.negated {input_bg}"""
    printp('\n# drmr:label negate-rev')
    for index, (sample, libraries) in enumerate(sorted(SAMPLES.items()), 1):
        if LIMIT_IO and index % LIMIT_IO == 0:
            printp("""\n# drmr:wait""")

        input_bg = '{sample}.read1.rev.pruned.coverage.bg'.format(**locals())
        printp(negate_template.format(**locals()))

    printp("""\n# drmr:wait""")

    track_template = """bedGraphToBigWig {input_bg} {chrom_sizes} {output_bw}"""

    for index, (sample, libraries) in enumerate(sorted(SAMPLES.items()), 1):
        if LIMIT_IO and index % LIMIT_IO == 0:
            printp("""\n# drmr:wait""")

        star_reference = STAR_REFERENCES[libraries[0]['reference_genome']]
        chrom_sizes = os.path.join(star_reference, 'chrNameLength.txt')

        for strand in ['fwd', 'rev']:
            input_bg = '{sample}.read1.{strand}.pruned.coverage.bg'.format(**locals())
            output_bw = input_bg.replace('.bg', '.bw')
            printp(track_template.format(**locals()))


if __name__ == '__main__':
    mkdir(WORK_PATH)
    mkdir(DATA_PATH)

    for source_file in iterate_all_source_files():
        dest = os.path.join(DATA_PATH, os.path.basename(source_file))
        symlink(source_file, dest)

    if os.path.exists(PIPELINE):
        os.unlink(PIPELINE)

    PIPELINE_FILE = open(PIPELINE, 'w')
    printp = functools.partial(print_to_pipeline, PIPELINE_FILE)

    printp('#!/bin/bash')
    printp('# -*- mode: sh; coding: utf-8 -*-\n')

    # We've seen marginal improvements when we trim adapters, so we do it by default,
    # but STAR's soft clipping is really almost as effective.
    trim_adapters()

    star()

    # Uncomment to create a single BAM for each library by merging all of its readgroup BAMs
    # merge_star_libraries()

    merge_star_samples()

    make_r1ss_tracks()  # run on samples, so requires merge_star_samples before this

    qorts()
    qorts_wig2bigwig()

    fastqc()
