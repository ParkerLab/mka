#!/usr/bin/env python

#
# This commands template needs customization wherever you see CHANGES
# REQUIRED. Where you see CHANGES RECOMMENDED, check that section to
# make sure it works for your pipeline.
#

from __future__ import print_function

import contextlib
import functools
import itertools
import os
import re


REFERENCE_ROOT = os.getenv('MKA_REFERENCE_ROOT', '/lab/data/reference')

prefix_reference_root = functools.partial(os.path.join, REFERENCE_ROOT)

STAR_REFERENCES = {
    'danforth': prefix_reference_root('mouse/danforth/index/STAR/current'),
    'hg19': prefix_reference_root('human/hg19/index/STAR/current'),
    'mm9': prefix_reference_root('mouse/mm9/index/STAR/current'),
    'mm9etn': prefix_reference_root('mouse/mm9etn/index/STAR/current/sjdbOverhang100'),
    'rn5': prefix_reference_root('rat/rn5/index/STAR/current'),
}

FASTQ_RE = re.compile('\.f(ast)?q(\.gz)?$')

ANALYSIS_NAME = "{{ANALYSIS_NAME}}"
DESCRIPTION = """{{DESCRIPTION}}"""
CONTROL_PATH = "{{CONTROL_PATH}}"
ANALYSIS_PATH = "{{ANALYSIS_PATH}}"
DATA_PATH = os.path.join(ANALYSIS_PATH, 'data')
WORK_PATH = os.path.join(ANALYSIS_PATH, 'work')
PIPELINE = os.path.join(ANALYSIS_PATH, 'pipeline')

FASTQC_DIR = os.path.join(WORK_PATH, 'fastqc')
STAR_DIR = os.path.join(WORK_PATH, 'star')
QORTS_DIR = os.path.join(WORK_PATH, 'qorts')

# By default, we use ionice and limit the number of particularly
# I/O-intensive jobs that run at once, to keep the machine
# responsive. If you're running on dedicated cluster nodes, you
# probably want to set this to False.
LIMIT_IO = 8


#
# Library dictionary
#

LIBRARIES = {{LIBRARIES}}


def maybe_gzip(filename, ioniced=False):
    """Compress a file with gzip."""
    template_data = {
        'f': filename,
        'ionice': ioniced and 'ionice -c 3 ' or ''
    }

    command_template = """if [ -r "{f}" ]; then {ionice}gzip -f "{f}"; elif [ -r "{f}".gz ]; then echo '"{f}" already gzipped.'; fi"""

    printp(command_template.format(**template_data))


def mkdir(dir, mode=0o0750):
    """Construct a directory hierarchy using the given permissions."""
    if not os.path.exists(dir):
        os.makedirs(dir, mode)


def print_to_pipeline(pipeline_file, text=None, timed=False, ioniced=False):
    """The primary function of all this: writing to a drmr script."""
    if text:
        if timed:
            pipeline_file.write('/usr/bin/time -v ')
        if ioniced:
            pipeline_file.write('ionice -c3 ')
        pipeline_file.write(text)
        pipeline_file.write('\n')


@contextlib.contextmanager
def working_directory(path):
    """Changes to the given directory, returning to the original working directory when the context block is exited."""
    original_directory = os.getcwd()
    try:
        os.chdir(path)
        yield
    finally:
        os.chdir(original_directory)


def symlink(source_path, dest_path, abs=False):
    """Create a symbolic link from the source_path to the dest_path, which can be a directory."""

    workdir = os.path.isdir(dest_path) and dest_path or os.path.dirname(dest_path)

    with working_directory(workdir):
        src = abs and os.path.abspath(source_path) or os.path.relpath(source_path)
        dest = dest_path
        dest_base = os.path.basename(dest)
        if os.path.isdir(dest_path):
            dest = os.path.join(dest_path, os.path.basename(src))
            if os.path.lexists(dest):
                os.unlink(dest)
            os.symlink(src, dest)
        else:
            mkdir(os.path.dirname(dest_path))
            if os.path.lexists(dest):
                os.unlink(dest)
            os.symlink(src, dest)
        return dest, dest_base


def rename_fastq(fastq, suffix=''):
    return FASTQ_RE.sub(fastq, suffix)


def iterate_library_source_files(library_name):
    """Generates a list of the library's original files."""
    library = LIBRARIES[library_name]
    for rg, files in sorted(library['readgroups'].items()):
        for f in files:
            yield f


def iterate_all_source_files():
    return itertools.chain(*[iterate_library_source_files(library_name) for library_name in sorted(LIBRARIES.keys()) ])


def iterate_library_files(library_name):
    """Generates a list of the library's files in DATA_PATH."""
    library = LIBRARIES[library_name]
    for rg, files in sorted(library['readgroups'].items()):
        for f in files:
            yield os.path.join(DATA_PATH, os.path.basename(f))


def iterate_all_files():
    return itertools.chain(*[iterate_library_files(library_name) for library_name in sorted(LIBRARIES.keys())])


def library_reference_genomes():
    return sorted(list(set(library['reference_genome'] for library_name, library in sorted(LIBRARIES.items()))))


def libraries_by_genome():
    libraries = {}
    for genome in library_reference_genomes():
        libraries[genome] = [library for library_name, library in sorted(LIBRARIES.items()) if library['reference_genome'] == genome]

    # return genome, libraries for each genom
    return sorted(libraries.items())


def make_read_group_path(library_name, readgroup, suffix=''):
    return '{library_name}___{readgroup}{suffix}'.format(**locals())


def make_read_group_header(library, id):
    read_group_components = {
        'ID': '{}___{}'.format(library['library'], id),

        # library
        'LB': library['library'],

        # sample
        'SM': library['sample'],

        # sequencing center name
        'CN': library['sequencing_center'],

        # ISO8601 date(time) of sequencing
        'DT': library['sequencing_date'],

        # platform (Illumina, Solid, etc. -- see the spec for valid values
        'PL': library['sequencing_platform'],

        # free-form description
        'DS': library['description'],
    }

    header = """@RG\\t{}""".format('\\t'.join('{}:{}'.format(k, v) for k, v in sorted(read_group_components.items()) if v))

    return header


def fastqc():
    """Run FastQC on all input libraries."""

    mkdir(FASTQC_DIR)

    printp("""\n#\n# run FastQC on initial data\n#""")
    printp("""\n# drmr:label fastqc""")
    printp("""\n# drmr:job time_limit=2h working_directory={}""".format(FASTQC_DIR))

    for index, f in enumerate(iterate_all_files(), 1):
        symlink(f, FASTQC_DIR)
        printp("""fastqc {}""".format(os.path.basename(f)), timed=True, ioniced=True)
        if LIMIT_IO and index % LIMIT_IO == 0:
            # limit the number of concurrent jobs to avoid thrashing the disk
            printp("""\n# drmr:wait""")


def star(mapping_template=None, threads=4, memory=32, max_star_instances=0):
    if mapping_template is None:
        mapping_template = """STAR --runThreadN {} --readFilesCommand gunzip -c --genomeDir {} --outSAMtype BAM SortedByCoordinate --outSAMunmapped Within KeepPairs --readFilesIn {}"""

    printp('\n# drmr:label star')

    index = 0
    for reference_genome, libraries in libraries_by_genome():
        star_reference = STAR_REFERENCES[reference_genome]

        for library in libraries:
            for rg, files in sorted(library['readgroups'].items()):
                files = sorted(files)
                star_dir = os.path.join(STAR_DIR, make_read_group_path(library['library'], rg))
                mkdir(star_dir)

                for f in files:
                    symlink(f, star_dir)

                fastq_files = [os.path.basename(f) for f in files]

                printp('\n# drmr:job time_limit=4h processors={} memory={}g working_directory={}'.format(threads, memory, star_dir))
                printp(mapping_template.format(threads, star_reference, ' '.join(fastq_files)), timed=True, ioniced=True)

                index += 1
                if max_star_instances and index % max_star_instances == 0:
                    # limit the number of concurrent jobs to avoid thrashing the disk
                    printp("""\n# drmr:wait""")

    printp("""\n# drmr:wait""")


def qorts(command_template=None, threads=4, memory=12):
    if command_template is None:
        qorts_command_template = """java -Xmx8g -jar $QORTS_JAR QC {} --generatePlots --title {} --chromSizes {} {} {} {}"""

    printp('\n# drmr:label qorts')

    # run QoRTs on each library's BAM
    for library_name, library in sorted(LIBRARIES.items()):
        star_reference = STAR_REFERENCES[library['reference_genome']]
        chrom_sizes = os.path.join(star_reference, 'chrNameLength.txt')
        gtf = os.path.join(star_reference, 'annotation.gtf')

        strand_options = ''
        library_strand = library['analysis_specific_options']['strand']
        if library_strand == 'fr-firststrand':
            strand_options = '--stranded'
        elif library_strand == 'fr-secondstrand':
            strand_options = '--stranded --stranded_fr_secondstrand'

        for rg, files in sorted(library['readgroups'].items()):
            read_group_dir = make_read_group_path(library_name, rg)
            star_dir = os.path.join(STAR_DIR, read_group_dir)
            bam = os.path.join(star_dir, 'Aligned.sortedByCoord.out.bam')

            qorts_dir = os.path.join(QORTS_DIR, read_group_dir)
            mkdir(qorts_dir)

            printp('\n# drmr:job time_limit=2h processors={} memory={}g working_directory={}'.format(threads, memory, qorts_dir))
            printp(qorts_command_template.format(strand_options, library_name, chrom_sizes, bam, gtf, qorts_dir), timed=True, ioniced=True)

    printp("""\n# drmr:wait""")

    printp('\n# drmr:label qorts-multi')

    # plot all the things
    decoder_filename = os.path.join(QORTS_DIR, 'decoder.txt')
    with open(decoder_filename, 'w') as decoder:
        decoder.write('sample.ID\n')
        for library_name, library in LIBRARIES.items():
            for rg in sorted(library['readgroups'].keys()):
                read_group_dir = make_read_group_path(library_name, rg)
                decoder.write('{}\n'.format(read_group_dir))

    mkdir(os.path.join(QORTS_DIR, ANALYSIS_NAME))

    printp('\n# drmr:job time_limit=2h processors={} memory={}g working_directory={}'.format(threads, memory, QORTS_DIR))
    printp("""qortsGenMultiQC.R {}/ {} {}/""".format(QORTS_DIR, decoder_filename, ANALYSIS_NAME))


def qorts_wig2bigwig(command_template=None, memory=1):
    if command_template is None:
        command_template = """ionice -c3 wigToBigWig {wig} {chrom_sizes} {bigwig}"""

    wigs = ['QC.wiggle.fwd.wig.gz', 'QC.wiggle.rev.wig.gz']
    bigwigs = ['QC.wiggle.fwd.bw', 'QC.wiggle.rev.bw']

    printp('\n# drmr:label qorts-wig2bigwig')

    for library_name, library in sorted(LIBRARIES.items()):
        star_reference = STAR_REFERENCES[library['reference_genome']]
        chrom_sizes = os.path.join(star_reference, 'chrNameLength.txt')
        for rg in sorted(library['readgroups'].keys()):
            read_group_dir = make_read_group_path(library_name, rg)
            star_dir = os.path.join(STAR_DIR, read_group_dir)
            bam = os.path.join(star_dir, 'Aligned.sortedByCoord.out.bam')

            qorts_dir = os.path.join(QORTS_DIR, read_group_dir)

            printp('\n# drmr:job time_limit=1h processors=1 memory={}g working_directory={}'.format(memory, qorts_dir))
            for wig, bigwig in zip(wigs, bigwigs):
                printp(command_template.format(**locals()))


if __name__ == '__main__':
    mkdir(WORK_PATH)
    mkdir(DATA_PATH)

    for source_file in iterate_all_source_files():
        dest = os.path.join(DATA_PATH, os.path.basename(source_file))
        symlink(source_file, dest)

    if os.path.exists(PIPELINE):
        os.unlink(PIPELINE)

    PIPELINE_FILE = open(PIPELINE, 'w')
    printp = functools.partial(print_to_pipeline, PIPELINE_FILE)

    printp('#!/bin/bash')
    printp('# -*- mode: sh; coding: utf-8 -*-\n')

    star()
    qorts()
    qorts_wig2bigwig()
    fastqc()
