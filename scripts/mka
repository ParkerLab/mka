#!/usr/bin/env python
# -*- coding: utf-8 -*-

#
# mka: make analysis
#
# Creates a control directory for an analysis, ready for customization
# and publication to GitHub.
#

from __future__ import print_function
import argparse
import csv
import json
import logging
import os.path
import pkgutil
import pprint
import re
import subprocess
import sys
import textwrap

import dateutil.parser
import jinja2
import mka
import mka.log
import mka.path


PROGRAM = 'mka'
PROGRAM_VERSION = mka.__version__

LIBRARY_FILENAME_PATTERN = re.compile(
    '^(?P<sample>.*?)'
    '___(?P<library>.*?)'
    '(?:___(?P<readgroup>.*?))?'  # optional
    '(?:___(?P<description>.*?))?'  # optional
    '\.(?P<pair_index>\d)*\.f(ast)*q.*'
)

ANALYSIS_SPECIFIC_TEMPLATE_PATTERN = re.compile('^(\w+)\.+(.+)')

ANALYSIS_SPECIFIC_LIBRARY_OPTIONS = {
    'atac-seq': {},
    'rna-seq': {
        'strand': {
            'choices': ['fr-unstranded', 'fr-firststrand', 'fr-secondstrand'],
            'default': 'fr-firststrand',
            'prompt': 'Library type',
        }
    }
}

URL_PREFIXES = {
    'SRR': 'http://www.ncbi.nlm.nih.gov/sra/?term=',
    'GSM': 'http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc='
}


class MissingTemplate(Exception):
    def __init__(self, template):
        self.template = template

    def __str__(self):
        return 'Template not found: {}'.format(self.template)


# yuck. Looking forward to being able to drop Python 2 someday.
try:
    input = raw_input
except:
    pass


def first(seq):
    return seq and next(iter(seq)) or None


def read_custom_template(path):
    content = None
    if os.path.isfile(path):
        try:
            content = open(path, 'rb').read()
            if not content:
                raise ValueError
        except:
            logging.error('Could not load content of custom template "{}"'.format(path))
            sys.exit(1)

    return content


def get_custom_template_directory():
    custom_template_directory = os.environ.get('{}_TEMPLATES'.format(PROGRAM.upper()))
    if custom_template_directory and not os.path.isdir(custom_template_directory):
        custom_template_directory = None
    return custom_template_directory


def get_templates():
    template_dirs = [d for d in [get_custom_template_directory(), os.path.join(mka.__path__[0], 'templates')] if d and os.path.isdir(d)]
    templates = set([])
    for template_dir in template_dirs:
        for filename in os.listdir(template_dir):
            if not ANALYSIS_SPECIFIC_TEMPLATE_PATTERN.match(filename):
                templates.add(os.path.basename(filename))
    return sorted(templates)


def get_supported_analyses():
    template_dirs = [d for d in [get_custom_template_directory(), os.path.join(mka.__path__[0], 'templates')] if d and os.path.isdir(d)]
    analyses = set([])
    for template_dir in template_dirs:
        for filename in os.listdir(template_dir):
            if filename.startswith('commands.'):
                analyses.add(filename.replace('commands.', ''))

    return sorted(analyses)


def find_template(template, analysis_type=None):
    """
    Search mka template directories for the given template.

    Searches the directory in the environment variable MKA_TEMPLATES,
    if it exists, then the default templates supplied with the mka
    package.

    Returns the content of the template, or raises MissingTemplate.
    """

    if analysis_type:
        sources = ['{}.{}'.format(template, analysis_type), template]
    else:
        sources = [template]

    custom_template_directory = get_custom_template_directory()
    templates = []
    for source in sources:
        if custom_template_directory:
            custom_template = os.path.join(custom_template_directory, source)
            if os.path.isfile(custom_template):
                templates.append(read_custom_template(custom_template))

        try:
            templates.append(pkgutil.get_data(PROGRAM, 'templates/{}'.format(source)))
        except:
            pass

    # turn the bytes content into strings
    templates = [t.decode('UTF=8') for t in templates if t]

    if not templates:
        raise MissingTemplate(template)

    return first(templates)


def write_template(template_context, template, destination_directory, analysis_type=None):
    template_content = find_template(template, analysis_type)
    template_environment = jinja2.Environment()
    try:
        t = template_environment.from_string(template_content)
        output = t.render(**template_context)
        destination_filename = os.path.join(destination_directory, template)
        with open(destination_filename, 'wb') as destination:
            destination.write(output.encode('utf-8'))
    except jinja2.exceptions.TemplateSyntaxError as e:
        logging.fatal('Error at line {} in template {}: {}'.format(e.lineno, template, e))
        offending_line = None
        i = 0
        for line in template_content.splitlines():
            i += 1
            if i == e.lineno:
                offending_line = line

        logging.fatal('Offending line:\n{}\n'.format(offending_line))
        raise


def write_templates(template_context, destination_directory, analysis_type=None):
    custom_template_directory = os.environ.get('{}_TEMPLATES'.format(PROGRAM.upper()))
    if custom_template_directory and not os.path.isdir(custom_template_directory):
        logging.warn('Ignoring invalid custom template directory "{}"'.format(custom_template_directory))

    for template in get_templates():
        write_template(template_context, template, destination_directory, analysis_type)


def parse_library_filename(library_file):
    sample = library = readgroup = description = pair_index = ''
    base = os.path.basename(library_file)

    m = LIBRARY_FILENAME_PATTERN.match(base)
    if m:
        sample = m.group('sample')
        library = m.group('library')

        # Defaulting the read group to 1 is OK even if the BAMs are
        # merged, because in the templates, the read group headers are
        # constructed by concatenating the library and the read group
        readgroup = m.group('readgroup') or 1

        description = (m.group('description') or '').replace('%20', ' ')
        pair_index = m.group('pair_index') or ''

    return {
        'sample': sample,
        'library': library,
        'readgroup': readgroup,
        'description': description,
        'pair_index': pair_index
    }


def read_runinfo(filename):
    """
    Reads the CSV containing run info.

    The UM DNA Sequencing Core supplies a CSV file with each run's
    results. They're usually named Run_<number>_<investigator>.csv.

    The files contain sample IDs, lanes, barcodes, projects (usually
    just the investigator's uniqname, unfortunately) and descriptions
    of each sample, as supplied when submitted for sequencing.
    """

    common_info = {
        'sequencing_center': 'UM DNA Sequencing Core',
    }

    illumina_applications = ['HiSeq']
    indata = False
    libraries = []
    with open(filename) as f:
        for line in f:
            line = line.strip()

            if indata:
                libraries.append(line)
            else:
                if line.startswith('Date,'):
                    sequencing_date = line.split(',')[1]
                    sequencing_date = dateutil.parser.parse(sequencing_date)
                    if sequencing_date:
                        common_info['sequencing_date'] = sequencing_date.date().isoformat()

                if line.startswith('Application,') and any([app in line for app in illumina_applications]):
                    common_info['sequencing_platform'] = 'ILLUMINA'

                if line.startswith('[Data]'):
                    indata = True

    info = {}
    reader = csv.DictReader(libraries, dialect='excel')
    for library in reader:
        id = library['Sample_ID'].replace('Sample_', '')
        info[id] = {
            'sample': id,
            'library': id,
            'description': library.get('Description', '')
        }
        info[id].update(common_info)

    return info


def get_default_metadata(json_file):
    defaults = json.load(open(json_file))

    # if the file is in the old format without sample/library/file-specific defaults, rewrite it
    if 'defaults' not in defaults:
        defaults = {
            'defaults': defaults,
            'samples': {},
            'libraries': {},
            'files': {}
        }

    return defaults


def get_metadata(metadata, attribute, prompt, transform=str):
    metadata[attribute] = transform(input(prompt.format(metadata[attribute])) or metadata[attribute])


def upper(s):
    return str(s).upper()


def describe_libraries(library_files, analysis_type=None, interactive=False, defaults_file=None, run_info_file=None):
    libraries = {}

    metadata = {
        'library': '',
        'sample': '',
        'readgroup': '',
        'pair_index': '',
        'url': '',
        'sequencing_center': '',
        'sequencing_date': '',
        'sequencing_platform': '',
        'sequencing_platform_model': '',
        'reference_genome': '',
        'description': '',
        'analysis_specific_options': {},
    }

    run_info = {}
    if run_info_file:
        logging.debug('Reading sequencing run information from {}'.format(run_info_file))
        run_info = read_runinfo(run_info_file)

    defaults = {}
    if defaults_file:
        defaults = get_default_metadata(defaults_file)

    for library_file in library_files:
        filename_metadata = parse_library_filename(library_file)
        metadata.update({k: v for k, v in filename_metadata.items() if k in metadata and v})

        metadata.update({k: v for k, v in defaults.get('defaults', {}).items() if k in metadata and v})
        metadata.update({k: v for k, v in defaults.get('samples', {}).get(filename_metadata.get('sample'), {}).items() if k in metadata and v})
        metadata.update({k: v for k, v in defaults.get('libraries', {}).get(filename_metadata.get('library'), {}).items() if k in metadata and v})
        metadata.update({k: v for k, v in defaults.get('files', {}).get(os.path.basename(library_file), {}).items() if k in metadata and v})

        if run_info:
            library_run_info = run_info[filename_metadata['sample']]
            if library_run_info:
                metadata.update({k: v for k, v in library_run_info.items() if k in metadata and v})

        if interactive:
            print('\nPlease supply metadata for library file {}:'.format(library_file))
            get_metadata(metadata, 'sample', '  sample ({}): ')
            get_metadata(metadata, 'library', '  library ({}): ')
            get_metadata(metadata, 'readgroup', '  read group ({}): ')
            get_metadata(metadata, 'pair_index', '  pair index -- 1 or 2 for paired end libraries, or blank ({}): ')
            get_metadata(metadata, 'reference_genome', '  reference genome, e.g. hg19 ({}): ')

        if not metadata.get('reference_genome'):
            while not metadata['reference_genome']:
                get_metadata(metadata, 'reference_genome', '  Please specify the reference genome: ')

        if metadata['library'][:3] in URL_PREFIXES:
            metadata['url'] = URL_PREFIXES[metadata['library'][:3]] + metadata['library']
        elif metadata['sample'][:3] in URL_PREFIXES:
            metadata['url'] = URL_PREFIXES[metadata['sample'][:3]] + metadata['sample']

        if interactive:
            get_metadata(metadata, 'description', '  description ({}): ')
            get_metadata(metadata, 'url', """  URL -- perhaps the library's SRA or GEO URL ({}): """)
            get_metadata(metadata, 'sequencing_platform', '  sequencing platform, e.g. ILLUMINA ({}): ', upper)
            get_metadata(metadata, 'sequencing_center', '  sequencing center ({}): ')
            get_metadata(metadata, 'sequencing_date', '  sequencing date ({}): ')

        date_ok = False
        sequencing_date = metadata['sequencing_date']
        while sequencing_date and not date_ok:
            sequencing_date = dateutil.parser.parse(sequencing_date)
            if sequencing_date:
                metadata['sequencing_date'] = sequencing_date.date().isoformat()
                date_ok = True
            else:
                logging.error('  Sorry, I could not interpret that date.')
                get_metadata(metadata, 'sequencing_date', '  sequencing date ({}): ')

        if interactive:
            for option, config in ANALYSIS_SPECIFIC_LIBRARY_OPTIONS[analysis_type].items():
                analysis_specific_options = metadata['analysis_specific_options']
                value = analysis_specific_options.get(option, '')
                prompt = '  {} {} ({}): '.format(
                    config['prompt'],
                    'choices' in config and '[{}]'.format(', '.join(config['choices'])) or '',
                    value or config.get('default', '')
                )
                value = input(prompt) or analysis_specific_options.get(option) or config.get('default')
                if 'choices' in config:
                    while value not in config['choices']:
                        value = input(prompt)

                metadata['analysis_specific_options'][option] = value

        library = metadata['library']
        if library in libraries:
            if libraries[library]['sample'] != metadata['sample']:
                raise ValueError('Library {} seems to belong to multiple samples in {}. Check your input.'.format(library, pprint.pformat(libraries, indent=4)))
        else:
            libraries[library] = {k: v for k, v in metadata.items() if k not in ['pair_index', 'readgroup']}
            libraries[library]['readgroups'] = {}  # read group ID -> list of files

        readgroups = libraries[library]['readgroups']
        if metadata['readgroup'] in readgroups:
            readgroups[metadata['readgroup']].append(library_file)
        else:
            readgroups[metadata['readgroup']] = [library_file]

    result = json.dumps(libraries, sort_keys=True, indent=4, ensure_ascii=False)
    return result


def initialize_git_repo(directory):
    os.chdir(directory)
    if not os.path.exists('.git'):
        git_email = ''
        try:
            git_email = subprocess.check_output(['git', 'config', 'user.email'])
        except:
            pass

        while not git_email:
            git_email = input('Enter the email address to use when committing to this repo: ')

        subprocess.check_output(['git', 'init'])
        subprocess.check_output(['git', 'config', '--local', 'user.email', git_email])
        subprocess.check_output(['git', 'add', '-A'])
        subprocess.check_output(['git', 'commit', '-m', 'initial commit'])


def parse_arguments():
    parser = mka.log.LoggingArgumentParser(
        prog=PROGRAM,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description=textwrap.dedent("""

        Given one or more DNA library files, creates an analysis pipeline from
        a template. Current templates support ATAC-seq and RNA-seq pipelines.

        You will be asked to describe each library file. All required metadata
        except the reference genome can be inferred if the files are named
        like this:

          sample___library___readgroup___description.read_pair_index.fastq.gz

        For example, this...

          GSM1155957___SRR891268___1___GM12878_ATACseq_50k_Rep1.1.fastq.gz

        ... would be interpreted as sample GSM1155957, library SRR891268, read
        group 1, described as GM12878_ATACseq_50k_Rep1, containing the first
        reads of a paired-end library.

        If samples and libraries look like GEO identifiers, then URLs for them
        can also be inferred and recorded.

        When using bwa or STAR to align reads, the library metadata will be
        used to add read groups to their output.

        You can also supply default metadata for the entire experiment with
        the --library-defaults option.

        If the data came from the UM DNA Sequencing Core, metadata for the
        libraries can also be parsed from the run info CSV files they supply.

        The precedence of metadata is: run info < library defaults < filenames.

        """)
    )

    parser.add_argument('-v', '--verbose', action='store_true', help='Talk more.')
    parser.add_argument('--version', action='version', version=PROGRAM_VERSION)

    parser.add_argument('-a', '--analysis-path', help='The directory where the analysis will run. If not specified, it will run in the control directory.')
    parser.add_argument('-d', '--description', help='A description of the analysis.')
    parser.add_argument('-i', '--interactive', action='store_true', help='Prompt for library metadata, instead of just inferring from filenames.')
    parser.add_argument('-l', '--library-defaults', type=mka.path.FilePath(require_readable=True), help='A JSON file containing default metadata to use for all libraries. This will override metadata parsed from the file supplied with the --run-info option.')
    parser.add_argument('-t', '--analysis-type', choices=get_supported_analyses(), help='The type of pipeline to create.')
    parser.add_argument('-n', '--name', required=False, help="""The name for the analysis (default: derived from the control directory).""")
    parser.add_argument('-r', '--run-info', type=mka.path.FilePath(require_readable=True), help='The path to a "run info" CSV file included with results from the UM DNA Sequencing Core. Library metadata will be supplemented with the run info. This metadata will be overridden with any supplied with --library-defaults or parsed from library FASTQ files.')
    parser.add_argument('control_path', help='The directory for the control files.')
    parser.add_argument('libraries', metavar='library', nargs='+', type=mka.path.FilePath(require_readable=True), help='One or more FASTQ files containing sequenced libraries.')

    return parser.parse_args()


def shquote(arg):
    return """'{}'""".format(arg.replace("'", "\'"))


def reconstruct_command_line(args):
    command_line = [sys.argv[0]]
    if args.analysis_path:
        command_line.append('--analysis-path ' + os.path.normpath(os.path.abspath(args.analysis_path)))

    if args.analysis_type:
        command_line.append('--analysis-type ' + shquote(args.analysis_type))

    if args.description:
        command_line.append('--description ' + shquote(args.description))

    if args.interactive:
        command_line.append('--interactive')

    if args.library_defaults:
        command_line.append('--library-defaults ' + os.path.normpath(os.path.abspath(args.library_defaults)))

    if args.name:
        command_line.append('--name ' + shquote(args.name))

    if args.run_info:
        command_line.append('--run-info ' + os.path.normpath(os.path.abspath(args.run_info)))

    command_line.append(os.path.normpath(os.path.abspath(args.control_path)))

    for library in args.libraries:
        command_line.append(os.path.normpath(os.path.abspath(library)))

    return ' \\\n    '.join(command_line)


if __name__ == '__main__':

    root = logging.getLogger()
    handler = mka.log.ColoringStreamHandler()
    handler.setFormatter(logging.Formatter(fmt='%(message)s'))
    root.addHandler(handler)
    root.setLevel(logging.INFO)

    args = parse_arguments()

    root.setLevel(args.verbose and logging.DEBUG or logging.INFO)

    control_path = os.path.abspath(args.control_path)
    analysis_path = args.analysis_path and os.path.abspath(args.analysis_path) or control_path

    libraries = []
    for library in args.libraries:
        library = os.path.abspath(library)
        if not os.path.exists(library):
            logging.fatal('Library file does not exist: {}'.format(library))
            sys.exit(1)

        if not os.path.isfile(library):
            logging.fatal('Library file is not a file: {}'.format(library))
            sys.exit(1)

        libraries.append(library)

    if not libraries:
        logging.fatal('Please supply at least one FASTQ file to analyze.')
        sys.exit(1)

    if os.path.exists(control_path):
        if not os.path.isfile(os.path.join(control_path, 'commands')):
            logging.fatal('Control path {} already exists, but does not look like it was generated by mka.'.format(control_path))
            sys.exit(1)

    analysis_name = args.name or os.path.basename(control_path)
    description = args.description and args.description.replace('"', r'\"').replace("'", "\'") or ''
    template_context = {
        'ANALYSIS_PATH': analysis_path,
        'ANALYSIS_NAME': analysis_name,
        'AUTHOR': os.getenv('LOGNAME'),
        'CONTROL_PATH': control_path,
        'DESCRIPTION': description,
        'LIBRARIES': describe_libraries(libraries, args.analysis_type, args.interactive, args.library_defaults, args.run_info),
        'MKA_COMMAND_LINE': reconstruct_command_line(args),
        'MKA_CWD': os.getcwd(),
    }

    if not os.path.exists(control_path):
        os.makedirs(control_path, 0o0755)

    write_templates(template_context, control_path, args.analysis_type)

    os.chmod(os.path.join(control_path, 'commands'), 0o0755)

    try:
        initialize_git_repo(control_path)
    except subprocess.CalledProcessError as e:
        logging.fatal('Could not initialize git repository: {}'.format(e))
        sys.exit(1)

    logging.info('\nYour analysis is ready in {}'.format(control_path))
